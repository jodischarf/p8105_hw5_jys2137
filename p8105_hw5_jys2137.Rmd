---
title: "Homework 5"
author: "jys2137"
date: "11/19/2021"
output: github_document
---

```{r setup, include=FALSE}
library(tidyverse)
library(p8105.datasets)
library(viridis)

knitr::opts_chunk$set(
	echo = TRUE,
	warning = FALSE,
	fig.width = 8, 
	fig.asp = .6,
  fig.height = 8,
	dpi = 300,
  out.width = "90%"
)
theme_set(theme_minimal() + theme(legend.position = "bottom"))
options(
  ggplot2.continuous.colour = "viridis",
  ggplot2.continuous.fill = "viridis"
)
scale_colour_discrete = scale_colour_viridis_d
scale_fill_discrete = scale_fill_viridis_d
```

The purpose of this file is to present the answers to Homework 5, an assignment reinforcing ideas in the **Iteration** topic for P8105. 

## Contents

**Problem 1**

* **1.1.** Data import
* **1.2.** Raw data description
* **1.3.** Data cleaning
* **1.4.** Total number of homicides and unsolved homicides
* **1.5.** Proportion of unsolved homicides in Baltimore
* **1.6.** Running `prop.test` for each of the cities in the dataset
* **1.7.** Creating a plot: Proportion of unsolved cases in 50 major US cities


**Problem 2**

* **2.1.** Importing the data
* **2.2.** Cleaning the data
* **2.3.** Spaghetti plot of observations over time
* **2.4.** A comment on differences between groups


**Problem 3**

* **3.1.** Loading in the `iris` dataset
* **3.2.** Writing a function
* **3.3.** Final checks

## Problem 1

This problem uses data on homicides in 50 large U.S. cities gathered by the _Washington Post_. 

#### 1.1. Data import

The code below imports the raw data made available through a GitHub repository.

```{r raw_import}
hom_url <- 'https://raw.githubusercontent.com/washingtonpost/data-homicides/master/homicide-data.csv'
hom_read <- read_csv(url(hom_url), na = c(" ","Unknown", "NA"))
```

#### 1.2. Raw data description

The **raw** dataset has **`r nrow(hom_read)`** observations and **`r ncol(hom_read)`** variables, including the _reported date_ of the homicide; the _name_, _race_, _age_, and _sex_ of the victim; and the _city_, _state_, and _longitude_ and _latitude_ of the homicide. Note that there is a possible data entry error of 1 observation reported as being in _Tulsa, AL_ (which does not exist; _Tulsa_ is in _OK_). 

#### 1.3. Data cleaning

The following code chunk **cleans the data** by:

* creating a `city_state` variable which combines city and state
* creating a `resolution` variable indicating if a case was closed with or without arrest
* excluding the 1 observation reported as being in _Tulsa, AL_

```{r data_clean_1}
homicide_df = 
  hom_read %>%
  mutate(
    city_state = str_c(city, state, sep = ", "),
    resolution = case_when(
      disposition == "Closed without arrest" ~ "unsolved",
      disposition == "Open/No arrest"        ~ "unsolved",
      disposition == "Closed by arrest"      ~ "solved")) %>% 
  relocate(city_state) %>% 
  filter(city_state != "Tulsa, AL") 
```

The **new, cleaned** dataset has **`r nrow(homicide_df)`** observations and **`r ncol(homicide_df)`** variables.

#### 1.4. Total number of homicides and unsolved homicides

The next code chunk summarizes _**within cities**_ to obtain:

1. The **total** number of homicides
2. The number of **unsolved** homicides (those for which the disposition is “Closed without arrest” or “Open/No arrest”).

```{r cities}
cities_df = 
  homicide_df %>% 
  select(city_state, disposition, resolution) %>% 
  group_by(city_state) %>% 
  summarize(
    unsolved = sum(resolution == "unsolved"),
    n = n())

knitr::kable(cities_df,
             col.names = c("City", "Unsolved Murders", "Total Murders"))

```

#### 1.5. Proportion of unsolved homicides in Baltimore

Focusing on the city of _Baltimore, MD_, we use the `prop.test` function to estimate the proportion of homicides that are unsolved and the `broom::tidy` function to pull the estimated proportion and confidence intervals from the resulting dataframe.

```{r baltimore}
baltimore_df =
  homicide_df %>% 
  filter(city_state == "Baltimore, MD")

baltimore_summary = 
  baltimore_df %>% 
  summarize(
    unsolved = sum(resolution == "unsolved"),
    n = n())

 baltimore_test =
   prop.test(
    x = baltimore_summary %>% pull(unsolved), 
    n = baltimore_summary %>% pull(n)) 
 
 
baltimore_test %>% 
  broom::tidy() %>% 
  select(-parameter, -alternative) %>% 
  rename(Estimate = estimate,
         Statistic = statistic,
         "p-value" = p.value,
         Method = method,
         "CI Lower Bound" = conf.low, 
         "CI Upper Bound" = conf.high) %>% 
  knitr::kable(digits = 3,
               caption = "_**Table 1: Estimate and CI of the Proportion of Unsolved Homicides In Baltimore**_",
               align = "ccccclc")
```

#### 1.6. Running `prop.test` for each of the cities in the dataset

Next, we run `prop.test` for each of the cities in the dataset, and extract both the **proportion of unsolved homicides** and the **confidence interval** for each. This is done within a “tidy” pipeline, making use of `purrr::map`, `purrr::map2`, and `unnest`.

```{r prop.test}
results_df = 
  cities_df %>% 
  mutate(test_results = map2(.x = unsolved, .y = n, ~prop.test(x = .x, n = .y)),
         tidy_results = map(test_results, broom::tidy)) %>% 
  select(-test_results) %>% 
  unnest(tidy_results) %>% 
  select(city_state, estimate, starts_with("conf"))
```

#### 1.7. Creating a plot: Proportion of unsolved cases in 50 major US cities

The code chunk below creates a **plot** that shows the *estimates** and **CIs** for each city – and uses `geom_errorbar` to add error bars based on the upper and lower limits. Cities are organized according to the proportion of unsolved homicides.

```{r plot_1}
results_df %>% 
  mutate(city_state = fct_reorder(city_state, estimate)) %>% 
  ggplot(aes(x = city_state, y = estimate)) + 
  geom_point(color = "darkred") + 
  geom_errorbar(aes(ymin = conf.low, ymax = conf.high)) + 
  theme(axis.text.x = element_text(angle = 90, vjust = 0.5, hjust = 1),
        plot.title = element_text(face = "bold.italic" )) +
  labs(title = "Figure 1: Proportion of unsolved cases in 50 major US cities", 
       y = "Estimated Proportion of Unsolved Homicides", 
       x = "City", 
       caption = "*Note: Error bars indicate 95% confidence interval")
```

_**Figure 1**_ illustrates that across the 50 major U.S. cities, rates at which homicides are solved vary greatly. Notably, **Chicago, IL** has a distinctly high proportion of unsolved homicides, at **over 0.7**. This unfortunately makes sense, as Chicago is a city with a high rate of homicides overall. In stark comparison, the lowest proportion of unsolved homicides is in **Richmond, VA**, at just above **0.25**.


## Problem 2

This problem involves data from a longitudinal study that included a control arm and an experimental arm. Data for each participant is included in a separate file, and file names include the `subject ID` and `arm`.

#### 2.1. Importing the data
First, we import the data from each separate file of the study through the following steps:

* Create a dataframe `files_df` that includes the names of all 20 files and the path to each file using `list.files`
* Use `purrr::map` for the paths and save the result as a new variable, `file_name`
* Import data using `read_csv`
* Use `unnest` to accommodate for the `map` results

```{r data_import_2}
files_df =
  tibble(
    files = list.files("./data/"),
    path = str_c("./data/", files)) %>% 
    mutate(file_name = map(path, read_csv, col_types = "dddddddd")) %>% 
    unnest()
```

#### 2.2. Cleaning the data
Now, we tidy the result of the previous data import through the following steps:

* Manipulate the file name to include `study arm` and subject ID (`subj_id`)
* Use string replacements for the file names
* Ensure weekly observations are tidy
* Change the dataframe from wide to long
* De-select variables that are not meaningful for our understanding of the data

```{r clean_data_2}
tidy_df = 
  files_df %>% 
  mutate(file_name = str_replace(files, ".csv", "")) %>% 
  mutate(file_name = str_replace(file_name, "./study_data/", "")) %>% 
  separate(file_name,
           into = c("study_arm", "subj_num"),
           sep = "_",
           remove = FALSE) %>% 
  rename(subj_id = file_name) %>% 
  pivot_longer(
    week_1:week_8,
    names_to = "week",
    values_to = "observations",
    names_prefix = "week_") %>% 
  mutate(week = as.numeric(week),
         study_arm = case_when(str_detect(files, "exp") ~ "Experiment",
                               str_detect(files, "con") ~ "Control")) %>% 
  select(study_arm, subj_num, subj = files, week, observations)
```
  
#### 2.3. Spaghetti plot of observations over time

Here, we make a **spaghetti plot** showing observations on each subject over time. For clarity, the **control group** is shown on the _left_ and the **experimental group** is shown on the _right_.

```{r spaghetti_plot}
tidy_df %>% 
  ggplot(aes(x = week, y = observations, group = subj, color = subj_num)) + 
  geom_point() + 
  geom_path() + 
  theme(plot.title = element_text(face = "bold.italic" )) + 
  labs(x = "Study Week",
       y = "Observed Measurement", 
       title = "Figure 2: Control vs. Experimental Arm", 
       color = "Subject ID Number") +
  facet_grid(~study_arm)
```

#### 2.4. A comment on differences between groups

Figure 2 illustrates a noticeable difference between groups. Subjects in the control group seem to stay rather consistent in their observed measurement over time, while subjects in the experimental group seem to have increasing observed measurements over time. 


## Problem 3

The purpose of this problem is to **fill in missing values** from the `iris` dataset.

#### 3.1. Loading in the iris dataset

The code chunk below **loads the iris dataset** from the tidyverse package and introduces some missing values in each column. 

```{r load_iris}
library(tidyverse)
set.seed(10)

iris_with_missing = iris %>% 
  map_df(~replace(.x, sample(1:150, 20), NA)) %>%
  mutate(Species = as.character(Species))
```

#### 3.2. Writing a function

Now, we write a function that takes a **vector as an argument** and **replaces missing** values such that two cases are addressed:

* For _numeric_ variables, missing values are filled in with the mean of non-missing values
* For _character_ variables, missing values are filled in with with "virginica"

The function subsequently returns the resulting vector.

```{r function_write}
missing_func = function(x) {
  
    if (is.character(x)) {
        x[is.na(x)] = "virginica"
    } else if (is.numeric(x)) {
        x[is.na(x)] = mean(x, na.rm = TRUE)
    } else stop("Input should be a numeric or character value")
  return(x)
}
```

The chunk below applies the function created above to the columns of `iris_with_missing` using a `map` statement. A `for loop` is also used to iterate across each column.

```{r function_map}
 for (i in 1:5) {
    iris_with_missing[i] = map(iris_with_missing[i], ~missing_func(.x))
  }
```

#### 3.3. Final checks

To make sure missing has been replaced correctly, we are conduct a **check** on the data frame to see if there are any **rows with NA values** in the dataframe.

```{r missing_check}
missing_check =
  iris_with_missing[rowSums(is.na(iris_with_missing)) > 0,]

missing_check
```

The dataframe looks like it does not include any more missing values. Finally, we can do a **test of the updated dataset**.

```{r final_check}
iris_with_missing %>%
  head()
```

Great, no missing values are found in the updated dataset.
